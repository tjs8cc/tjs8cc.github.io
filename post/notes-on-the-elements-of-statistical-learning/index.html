<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.42.1" />
  <meta name="author" content="Tyler Smith">

  
  
  
  
    
      
    
  
  <meta name="description" content="Purpose The Bias-Variance Tradeoff Tree-based Methods   Purpose The Elements of Statistical Learning by Hastie et. al. is the best resource I’ve found for understanding modern statistical and machine learning techniques. These techniques aim to answer problems of the following kind: what’s the best approximation to the conditional expectation function \(f(x) = E[Y|X]\)? Or more generally, what is our best guess for \(Y\) given what we know about \(X\)?">

  
  <link rel="alternate" hreflang="en-us" href="/post/notes-on-the-elements-of-statistical-learning/">

  


  

  
  
  <meta name="theme-color" content="#0095eb">
  
  
  
  
    
  
  
    
    
      
        <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
      
    
  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">
  
  
  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700%7cRoboto:400,400italic,700%7cRoboto&#43;Mono">
  
  <link rel="stylesheet" href="/styles.css">
  

  

  
  <link rel="alternate" href="/index.xml" type="application/rss+xml" title="Tyler Smith">
  <link rel="feed" href="/index.xml" type="application/rss+xml" title="Tyler Smith">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="/post/notes-on-the-elements-of-statistical-learning/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@tjs8cc">
  <meta property="twitter:creator" content="@tjs8cc">
  
  <meta property="og:site_name" content="Tyler Smith">
  <meta property="og:url" content="/post/notes-on-the-elements-of-statistical-learning/">
  <meta property="og:title" content="Notes on The Elements of Statistical Learning | Tyler Smith">
  <meta property="og:description" content="Purpose The Bias-Variance Tradeoff Tree-based Methods   Purpose The Elements of Statistical Learning by Hastie et. al. is the best resource I’ve found for understanding modern statistical and machine learning techniques. These techniques aim to answer problems of the following kind: what’s the best approximation to the conditional expectation function \(f(x) = E[Y|X]\)? Or more generally, what is our best guess for \(Y\) given what we know about \(X\)?">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2018-06-21T00:00:00&#43;00:00">
  
  <meta property="article:modified_time" content="2018-06-21T00:00:00&#43;00:00">
  

  
  

  <title>Notes on The Elements of Statistical Learning | Tyler Smith</title>

</head>
<body id="top" data-spy="scroll" data-target="#toc" data-offset="71" >

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">Tyler Smith</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      
      <ul class="nav navbar-nav navbar-right">
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#resume">
            
            <span>Resume</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  


  <div class="article-container">
    <h1 itemprop="name">Notes on The Elements of Statistical Learning</h1>

    

<div class="article-metadata">

  <span class="article-date">
    
    <time datetime="2018-06-21 00:00:00 &#43;0000 UTC" itemprop="datePublished dateModified">
      Jun 21, 2018
    </time>
  </span>
  <span itemscope itemprop="author publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Tyler Smith">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    3 min read
  </span>
  

  
  

  
  
  
  

  
  
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=Notes%20on%20The%20Elements%20of%20Statistical%20Learning&amp;url=%2fpost%2fnotes-on-the-elements-of-statistical-learning%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=%2fpost%2fnotes-on-the-elements-of-statistical-learning%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-facebook"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=%2fpost%2fnotes-on-the-elements-of-statistical-learning%2f&amp;title=Notes%20on%20The%20Elements%20of%20Statistical%20Learning"
         target="_blank" rel="noopener">
        <i class="fa fa-linkedin"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=%2fpost%2fnotes-on-the-elements-of-statistical-learning%2f&amp;title=Notes%20on%20The%20Elements%20of%20Statistical%20Learning"
         target="_blank" rel="noopener">
        <i class="fa fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=Notes%20on%20The%20Elements%20of%20Statistical%20Learning&amp;body=%2fpost%2fnotes-on-the-elements-of-statistical-learning%2f">
        <i class="fa fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>


    <div class="article-style" itemprop="articleBody">
      <div id="TOC">
<ul>
<li><a href="#purpose">Purpose</a></li>
<li><a href="#the-bias-variance-tradeoff">The Bias-Variance Tradeoff</a></li>
<li><a href="#tree-based-methods">Tree-based Methods</a></li>
</ul>
</div>

<div id="purpose" class="section level1">
<h1>Purpose</h1>
<p><em>The Elements of Statistical Learning</em> by Hastie et. al. is the best resource
I’ve found for understanding modern statistical and machine learning
techniques. These techniques aim to answer problems of the following kind:
what’s the best approximation to the conditional expectation function
<span class="math inline">\(f(x) = E[Y|X]\)</span>? Or more generally, what is our best guess for <span class="math inline">\(Y\)</span> given what
we know about <span class="math inline">\(X\)</span>? Where ‘best’ can be defined in a number of ways, but
usually results in trying to approximate the conditional expectation function
(CEF). A critical assumption made in most of these techniques is that the
error is additive, so that the general model is <span class="math display">\[ Y = f(x) + \epsilon\]</span> along
with some assumptions about how <span class="math inline">\(\epsilon\)</span> is distributed. The assumptions on
<span class="math inline">\(\epsilon\)</span> should not be considered an after thought; often they are the most
important part of the model, especially when you’re interested in causal
questions (see <em>Mostly Harmless Econometrics</em>).</p>
<p>I want to summarize and replicate some of the results from their book (as well
as from their introductory book <em>An Introduction to Statistical Learning</em>)
to get a better handle on these techniques.</p>
</div>
<div id="the-bias-variance-tradeoff" class="section level1">
<h1>The Bias-Variance Tradeoff</h1>
<p>A theme running throughout <em>The Elements of Statistical Learning</em> is that any
estimation strategy must make a choice between how much bias or variance to
tolerate. Hastie et. al. have a nice discussion of this in their overview
chapter. They note that the mean squared error of an estimator is
<span class="math display">\[MSE(\hat{f}(x)) = Var(\hat{f}(x))+ [Bias(\hat{f}(x))]^2 + Var(\epsilon)\]</span>
The variance of <span class="math inline">\(\epsilon\)</span> is the irreducible error. There’s nothing that our
model can do to reduce this error. The bias and variance of <span class="math inline">\(\hat{f}(x)\)</span> can
however be reduced. Roughly, the bias results from misspecifying the CEF and the
variance of <span class="math inline">\(\hat{f}(x)\)</span> is due to sampling error, which can be reduced by
increasing the sample size. So it would seem obvious that we should choose
correctly specified models to get the best predictions, but what Hastie et.
al. show is that often times we can do better by intentionally biasing our model
if the variance is reduced enough in exchange.</p>
</div>
<div id="tree-based-methods" class="section level1">
<h1>Tree-based Methods</h1>
<p>The tree method is very intuitive. We would like to approximate a function <span class="math inline">\(f(x)\)</span> by
partitioning its domain and approximating the function over each region with a
constant equal to the average value of <span class="math inline">\(y\)</span> in each region. Deciding how to
partition the domain results in different trees.</p>
<pre class="r"><code>Carseats = data.frame(Carseats, High)
tree.carseats = tree(High ~ ShelveLoc + Price, Carseats )
summary(tree.carseats)</code></pre>
<pre><code>## 
## Classification tree:
## tree(formula = High ~ ShelveLoc + Price, data = Carseats)
## Number of terminal nodes:  10 
## Residual mean deviance:  0.9325 = 363.7 / 390 
## Misclassification error rate: 0.2325 = 93 / 400</code></pre>
<pre class="r"><code>plot(tree.carseats, type = c(&quot;uniform&quot;))
text(tree.carseats, pretty = 0)</code></pre>
<p><img src="/post/2018-06-21-notes-on-the-elements-of-statistical-learning_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
</div>

    </div>

    


<div class="article-tags">
  
  <a class="btn btn-primary btn-outline" href="/tags/statistics/">statistics</a>
  
</div>




    
    

    

    


  </div>
</article>

<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; 2018 &middot; 

      Powered by the
      <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
      <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    

    

    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>
    
    
    <script src="/js/hugo-academic.js"></script>
    

    
    
      
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
      

      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
    </script>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML" integrity="sha512-tOav5w1OjvsSJzePRtt2uQPFwBoHt1VZcUq8l8nm5284LEKE9FSJBQryzMBzHxY5P0zRdNqEcpLIRVYFNgu1jw==" crossorigin="anonymous"></script>
    
    

  </body>
</html>

