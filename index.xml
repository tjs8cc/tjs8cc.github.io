<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tyler Smith on Tyler Smith</title>
    <link>/</link>
    <description>Recent content in Tyler Smith on Tyler Smith</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Wed, 20 Apr 2016 00:00:00 -0400</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Predicting Blood Donations: Part 2</title>
      <link>/post/predicting-blood-donations-part-2/</link>
      <pubDate>Mon, 02 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/predicting-blood-donations-part-2/</guid>
      <description>&lt;div id=&#34;purpose&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Purpose&lt;/h2&gt;
&lt;p&gt;This post shows some basic modeling methods in R. Specifically, I consider
modeling blood donation data with a linear and logit model. These models
are well established an easy to inerpret. In a future post I’ll explore some
more flexible models that might improve prediction.&lt;/p&gt;
&lt;div id=&#34;linear-regression-with-lm&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Linear regression with &lt;code&gt;lm&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;A linear model is the simplest a best understood method for modeling
data, as well as a building block for more sophisticated methods. So
I’ll start with it here, but note that it may not be appropriate for
this data investigation since the outcome variable is a binary
categorical variable.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Innovation at Universities</title>
      <link>/post/innovation-at-universities/</link>
      <pubDate>Fri, 29 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/innovation-at-universities/</guid>
      <description>&lt;p&gt;Who should have the rights to innovations created at a university:
professors or the university? A natural answer would be the professors;
they came up with the idea in the first place. But that’s not how it works
in the US.&lt;/p&gt;
&lt;p&gt;In the US, professors have rights to roughly a third of the income from
patents and ventures started at universities. This arrangement is motivated
by the fact that universities are a crucial partner in innovation.
Universities supply the basic resources and environment needed to incubate
risky ideas before it is clear they are viable commercial ventures. Those
resources are costly. If universities aren’t compensated enough, they might
not provide enough support for professors to bring their ideas to fruition.&lt;/p&gt;
&lt;p&gt;In 1980 the US passed the Bayh-Dole Act with the hope that providing more
compensation to universities would encourage more innovation. This
eventually inspired several European countries to do the same. In 2003,
Norway ended a policy known as “professor’s privilege” that
gave professors full rights to new business ventures and intellectual
property they created. Instead, Norway adopted a US-style policy that
transferred two-thirds of these rights to the university. Economists Hans K.
Hvide and Benjamin F. Jones observed the effects of this new US-style
policy in their paper &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/aer.20160284&#34;&gt;&lt;em&gt;University Innovation and the Professor’s
Privilege&lt;/em&gt;&lt;/a&gt;,
published in the July 2018 issue of the AER.&lt;/p&gt;
&lt;p&gt;Hvide and Jones found that not only did the policy fail to encourage
innovation; both entrepreneurship and patenting rates by university
researchers were cut in half after the reform. Furthermore, measures of
quality for university start-ups and patents declined as well. While the
authors perform a more detailed quantitative analysis in their paper,
specifically a differences-in-differences regression model, you can still
see a sharp contrast between university and non-university start-ups in
the graph below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-29-innovation-at-universities_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Both university and non-university start-ups decline in 2003, the year that
“professor’s privilege” ended, but the university start-up decline is
sharper and fails to recover. Although not as dramatic of a difference as
start-ups, the total number of university patents declines more sharply after
2003 than non-university patents.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-29-innovation-at-universities_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The authors find this general pattern to hold under a number of different
assumptions. To supplement their quantitative findings, they also conducted
a survey of university inventors. The survey indicated that university
inventors had a largely negative view of the reforms. Some respondents felt
that the university’s ownership share did not reflect its contribution to
commercializing an idea. To give a representative quote, one respondent said:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;The university contributed little, but was entitled to a substantial
income share.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;You can see a summary of the respondents’ views of the reform in the table
below. The authors caution that the response rate was only 22.3%.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Respondents&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Unknown&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Positive&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Negative&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Effect on self&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;56&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;34&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;15&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Effect on colleagues&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;56&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;42&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Overall, the authors’ findings suggest that the investment that researchers
put in to commercializing innovative ideas are more important than what
universities bring to a new venture. But these findings are limited to
Norway. Locating the right balance between a university’s interests and a
researcher’s interests in order to encourage new technologies and companies
is ultimately an empirical problem. Circumstances might vary widely from
country to country or from university to university.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Measuring Measure for Measure</title>
      <link>/post/measuring-measure-for-measure/</link>
      <pubDate>Mon, 25 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/measuring-measure-for-measure/</guid>
      <description>&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;purpose&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Purpose&lt;/h1&gt;
&lt;p&gt;This is a work in progress, but I’d like to post a summary
and my thoughts on Shakespeare’s &lt;em&gt;Measure for Measure&lt;/em&gt; and then
use it to perform some basic Natural Language Processing (NLP)&lt;/p&gt;
&lt;p&gt;Note: if I end up doing a sentiment analysis, the sentiment libraries
I’m aware of would not really be appropriate for early modern
English.&lt;/p&gt;
&lt;div id=&#34;overview&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;word-frequencies&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Word Frequencies&lt;/h2&gt;
&lt;p&gt;Load &lt;em&gt;Measure for Measure&lt;/em&gt; from the Gutenberg Project:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mm &amp;lt;- gutenberg_download(1126)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Tokenize&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidy_mm &amp;lt;- mm %&amp;gt;%
  unnest_tokens(word, text) %&amp;gt;%
  anti_join(stop_words)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;count&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mm_count &amp;lt;- tidy_mm %&amp;gt;%
              count(word, sort = TRUE)
kable(head(mm_count)) %&amp;gt;%
  kable_styling(bootstrap_options = c(&amp;quot;striped&amp;quot;, &amp;quot;hover&amp;quot;, &amp;quot;condensed&amp;quot;, &amp;quot;responsive&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped table-hover table-condensed table-responsive&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
word
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
n
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
duke
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
261
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
angelo
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
168
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
isabella
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
159
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
sir
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
135
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
lucio
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
132
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
provost
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
116
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Notes on The Elements of Statistical Learning</title>
      <link>/post/notes-on-the-elements-of-statistical-learning/</link>
      <pubDate>Thu, 21 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/notes-on-the-elements-of-statistical-learning/</guid>
      <description>&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#purpose&#34;&gt;Purpose&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-bias-variance-tradeoff&#34;&gt;The Bias-Variance Tradeoff&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#note-statistics-vs-econometrics&#34;&gt;Note: Statistics vs Econometrics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tree-based-methods&#34;&gt;Tree-based Methods&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;purpose&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Purpose&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;The Elements of Statistical Learning&lt;/em&gt; by Hastie et. al. is the best resource
I’ve found for understanding modern statistical and machine learning
techniques. These techniques aim to answer problems of the following kind:
what’s the best approximation to the conditional expectation function
&lt;span class=&#34;math inline&#34;&gt;\(f(x) = E[Y|X]\)&lt;/span&gt;? Or more generally, what is our best guess for &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; given what
we know about &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;? Where ‘best’ can be defined in a number of ways, but
usually results in trying to approximate the conditional expectation function
(CEF). A critical assumption made in most of these techniques is that the
error is additive, so that the general model is &lt;span class=&#34;math display&#34;&gt;\[ Y = f(x) + \epsilon\]&lt;/span&gt; along
with some assumptions about how &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; is distributed. The assumptions on
&lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; should not be considered an after thought; often they are the most
important part of the model, especially when you’re interested in causal
questions (see &lt;em&gt;Mostly Harmless Econometrics&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;I want to summarize and replicate some of the results from their book (as well
as from their introductory book &lt;em&gt;An Introduction to Statistical Learning&lt;/em&gt;)
to get a better handle on these techniques.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-bias-variance-tradeoff&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Bias-Variance Tradeoff&lt;/h1&gt;
&lt;p&gt;A theme running throughout &lt;em&gt;The Elements of Statistical Learning&lt;/em&gt; is that any
estimation strategy must make a choice between how much bias or variance to
tolerate. Hastie et. al. have a nice discussion of this in their overview
chapter. They note that the mean squared error of an estimator is
&lt;span class=&#34;math display&#34;&gt;\[MSE(\hat{f}(x)) = Var(\hat{f}(x))+ [Bias(\hat{f}(x))]^2 + Var(\epsilon)\]&lt;/span&gt;
The variance of &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; is the irreducible error. There’s nothing that our
model can do to reduce this error. The bias and variance of &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(x)\)&lt;/span&gt; can
however be reduced. Roughly, the bias results from misspecifying the CEF and the
variance of &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(x)\)&lt;/span&gt; is due to sampling error, which can be reduced by
increasing the sample size. So it would seem obvious that we should choose
correctly specified models to get the best predictions, but what Hastie et.
al. show is that often times we can do better by intentionally biasing our
model if the variance is reduced enough in exchange.&lt;/p&gt;
&lt;hr /&gt;
&lt;div id=&#34;note-statistics-vs-econometrics&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Note: Statistics vs Econometrics&lt;/h3&gt;
&lt;p&gt;Econometrics places a lot of emphasis on obtaining unbiased and consitent
estimators (see Introduction to Econometrics by Stock and Watson).
That doesn’t seem to be the case in statistics. I think this is due to
different interests: prediction vs causality. It’s certainly more common
to hear econometricians say they are more interested in causality. In
the linear regression model &lt;span class=&#34;math display&#34;&gt;\[y = \beta x + \epsilon\]&lt;/span&gt; an economist
wants the best guess for &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;, which would tell us what &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; would be
if we changed &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. More generally, an economist want the best guess for
&lt;span class=&#34;math inline&#34;&gt;\(f(x+\delta)-f(x)\)&lt;/span&gt;. In contrast, a statistician wants the best guess
for &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;. Although, the distinction doesn’t seem that significant. Maybe a
better way of describing the difference is to say that economists care
more about what we don’t know, i.e. what’s in the error term.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;tree-based-methods&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Tree-based Methods&lt;/h1&gt;
&lt;p&gt;The tree method is very intuitive. We would like to approximate a function &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; by
partitioning its domain and approximating the function over each region with a
constant equal to the average value of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; in each region. Deciding how to
partition the domain results in different trees.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Carseats = data.frame(Carseats, High)
tree.carseats = tree(High ~ ShelveLoc + Price, Carseats )
summary(tree.carseats)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Classification tree:
## tree(formula = High ~ ShelveLoc + Price, data = Carseats)
## Number of terminal nodes:  10 
## Residual mean deviance:  0.9325 = 363.7 / 390 
## Misclassification error rate: 0.2325 = 93 / 400&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(tree.carseats, type = c(&amp;quot;uniform&amp;quot;))
text(tree.carseats, pretty = 0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-21-notes-on-the-elements-of-statistical-learning_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Predicting Blood Donations: Part 1</title>
      <link>/post/predicting-blood-donations-part-1/</link>
      <pubDate>Thu, 21 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/predicting-blood-donations-part-1/</guid>
      <description>&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;purpose&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Purpose&lt;/h2&gt;
&lt;p&gt;This post shows some basic R usage, data cleaning and exploration, and
visualization methods. In
&lt;a href=&#34;https://tjs8cc.github.io/post/predicting-blood-donations-part-2/&#34;&gt;Part2&lt;/a&gt;,
I explore some quantitative methods.&lt;/p&gt;
&lt;div id=&#34;obtaining-data-in-r&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Obtaining data in R&lt;/h3&gt;
&lt;p&gt;R has a useful function called &lt;code&gt;download.file&lt;/code&gt;, which I show here for future
reference. Using the option &lt;code&gt;method = curl&lt;/code&gt;, and maybe the &lt;code&gt;extra&lt;/code&gt; option
to pass additional arguments, I can download data from the DrivenData
website.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;training_data_url &amp;lt;- &amp;quot;https://s3.amazonaws.com/drivendata/data/2/public/
                      9db113a1-cdbe-4b1c-98c2-11590f124dd8.csv&amp;quot;
train_file &amp;lt;- &amp;quot;../../data/training.csv&amp;quot;
if(!file.exists(train_file)){
  download.file(training_data_url, train_file, method = &amp;quot;curl&amp;quot;)
}

test_data_url &amp;lt;- &amp;quot;https://s3.amazonaws.com/drivendata/data/2/public/
                  5c9fa979-5a84-45d6-93b9-543d1a0efc41.csv&amp;quot;
test_file &amp;lt;- &amp;quot;../../data/test.csv&amp;quot;
if(!file.exists(test_file)){
  download.file(test_data_url, test_file, method = &amp;quot;curl&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is pretty small data set with only 5 variable: 4 independent
variables and 1 binary dependent variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;training &amp;lt;- read.csv(&amp;quot;../../data/training.csv&amp;quot;, row.names = 1)
kable(data.frame(Names = c(&amp;quot;last&amp;quot;, &amp;quot;number&amp;quot;, &amp;quot;volume&amp;quot;, &amp;quot;first&amp;quot;, &amp;quot;donated07&amp;quot;),
                 Description = names(training))) %&amp;gt;%
  kable_styling(bootstrap_options = c(&amp;quot;striped&amp;quot;, &amp;quot;hover&amp;quot;, &amp;quot;condensed&amp;quot;, &amp;quot;responsive&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped table-hover table-condensed table-responsive&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Names
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Description
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
last
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Months.since.Last.Donation
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
number
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Number.of.Donations
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
volume
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Total.Volume.Donated..c.c..
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
first
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Months.since.First.Donation
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
donated07
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Made.Donation.in.March.2007
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Next, to look at the first few rows of data, I run:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;names(training) &amp;lt;- c(&amp;quot;last&amp;quot;, &amp;quot;number&amp;quot;, &amp;quot;volume&amp;quot;, &amp;quot;first&amp;quot;, &amp;quot;donated07&amp;quot;)
kable(head(training)) %&amp;gt;%
  kable_styling(bootstrap_options = c(&amp;quot;striped&amp;quot;, &amp;quot;hover&amp;quot;, &amp;quot;condensed&amp;quot;, &amp;quot;responsive&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped table-hover table-condensed table-responsive&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
last
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
number
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
volume
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
first
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
donated07
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
619
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
50
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
98
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
664
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
13
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3250
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
28
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
441
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
16
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
35
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
160
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
45
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
358
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
24
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
77
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
335
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr /&gt;
&lt;div id=&#34;read.csv-vs-read_csv&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;code&gt;read.csv&lt;/code&gt; vs &lt;code&gt;read_csv&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;I chose to use &lt;code&gt;read.csv&lt;/code&gt; here since this data set came with row names. But
in general, &lt;code&gt;read_csv&lt;/code&gt; loads data faster and reads the data into a &lt;code&gt;tibble&lt;/code&gt;,
and &lt;code&gt;tibbles&lt;/code&gt; do not have row names.&lt;code&gt;read.csv&lt;/code&gt; reads the data into a
&lt;code&gt;data.frame&lt;/code&gt;.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exploring-the-data-in-r&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Exploring the data in R&lt;/h3&gt;
&lt;p&gt;First lets check the summary statistics and correlations with outcome
variable:&lt;/p&gt;
&lt;!-- write a custome function for this --&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor_row &amp;lt;- c(&amp;quot;Corr.&amp;quot;, round(cor(training$donated07, training), 4))
training %&amp;gt;%
  summary() %&amp;gt;%
  tidy() %&amp;gt;%
  mutate(Freq = as.character(Freq)) %&amp;gt;%
  separate(Freq, c(&amp;quot;statistic&amp;quot;, &amp;quot;value&amp;quot;), &amp;quot;:&amp;quot;) %&amp;gt;%
  select(-one_of(&amp;quot;Var1&amp;quot;)) %&amp;gt;%
  rename(variable = Var2) %&amp;gt;%
  spread(variable, value) %&amp;gt;%
  rbind(cor_row) %&amp;gt;%
  kable() %&amp;gt;%
  kable_styling(bootstrap_options = c(&amp;quot;striped&amp;quot;, &amp;quot;hover&amp;quot;, &amp;quot;condensed&amp;quot;, &amp;quot;responsive&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped table-hover table-condensed table-responsive&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
statistic
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
last
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
number
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
volume
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
first
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
donated07
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1st Qu.
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2.000
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2.000
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
16.00
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
3rd Qu.
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
14.000
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
7.000
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1750
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
49.25
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Max.
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
74.000
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
50.000
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
12500
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
98.00
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1.0000
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mean
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
9.439
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
5.427
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1357
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
34.05
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0.2396
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Median
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
7.000
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
4.000
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
28.00
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Min.
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0.000
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1.000
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
250
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2.00
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Corr.
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
-0.2612
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0.2206
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0.2206
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
-0.0198
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Next, let’s graph the histograms and scatter plots of the co-variates.
A quick way to do this is to plot a scatter matrix. I’ll color by the outcome
variable &lt;code&gt;donated07&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggpairs(training, aes(colour=factor(donated07)), 
        diag = list(continuous = wrap(&amp;#39;barDiag&amp;#39;, bins = 20)),
        columns = 1:(ncol(training)-1), progress = F) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-21-predicting-blood-donations-part-1_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Notice that &lt;code&gt;number&lt;/code&gt; and &lt;code&gt;volume&lt;/code&gt; are perfectly correlated, which means that
one of them needs to be dropped in any regression. The remaining co-variates
do not look strongly correlated. It also looks like &lt;code&gt;first&lt;/code&gt; might be top-coded
at 100 months.&lt;/p&gt;
&lt;p&gt;People who were more likely to donate in 2007 seemed to donate more in
general, either by frequency or volume. Their last donation also seemed to
be more recent. The scatter matrix doesn’t reveal much else to my eye. In
part 2, I’ll consider quantitative methods to estimate the strength of
these qualitative observations.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;note-data-pre-processing&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Note: data pre-processing&lt;/h3&gt;
&lt;p&gt;To remove duplicate rows, we can use the &lt;code&gt;unique&lt;/code&gt; or &lt;code&gt;distinct&lt;/code&gt;
functions. &lt;code&gt;unique&lt;/code&gt; returns all the unique rows of a data frame.
&lt;code&gt;distinct&lt;/code&gt; is an optimized version of &lt;code&gt;unique&lt;/code&gt; from the &lt;code&gt;dplyr&lt;/code&gt;
package. &lt;code&gt;distnct&lt;/code&gt; can also drop duplicate values based on
any number of columns. Or to return the duplicated rows we could
use &lt;code&gt;duplicated&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;training[duplicated(training), ]&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;p&gt;It’s easy to forget that row names aren’t considered when checking for
duplicates. So, in this case, we would want to include an &lt;code&gt;id&lt;/code&gt; column and
then remove any duplcates. This is also a point in favor of using tibbles.&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;Removing missing values is simple in R too. We can use &lt;code&gt;complete.cases&lt;/code&gt; or
&lt;code&gt;na.omit&lt;/code&gt;. &lt;code&gt;na.omit&lt;/code&gt; returns a data frame without any rows containing
&lt;code&gt;NA&lt;/code&gt;. And &lt;code&gt;complete.cases&lt;/code&gt; returns the indices of rows without &lt;code&gt;NA&lt;/code&gt;, so
it can be used to return rows with &lt;code&gt;NA&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;training[!complete.cases(training),]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] last      number    volume    first     donated07
## &amp;lt;0 rows&amp;gt; (or 0-length row.names)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Clearly, the &lt;code&gt;training&lt;/code&gt; data set in this example doesn’t contain any missing
values.&lt;/p&gt;
&lt;p&gt;Finally, it’s a good idea to check for any impossible values. Writing a
custom function for a data set is a good way to document any changes you’ve
made. For example, we know that every variable in this data set must be
non-negative and &lt;code&gt;first &amp;gt;= last&lt;/code&gt;. So we could write the following function
to remove any rows that violate these conditions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;clean &amp;lt;- function(x) {
  if (!is.data.frame(x)) stop(&amp;#39;x must be a data.frame.&amp;#39;)
  x_cleaned &amp;lt;- x %&amp;gt;%
    filter(first &amp;gt;= last) %&amp;gt;%
    filter_all(any_vars(. &amp;gt;= 0))
  return(x_cleaned)
}
training_cleaned &amp;lt;- clean(training)
sum(!training_cleaned==training)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that all of the observations satisfy the conditions
we put in &lt;code&gt;clean&lt;/code&gt;.&lt;/p&gt;
&lt;hr /&gt;
&lt;!-- Next, let&#39;s see if there may be any problem points in the regressors,  --&gt;
&lt;!-- check for high leverage points (lof - dbscan pkg) --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- lof_score &lt;- training %&gt;% select(-one_of(c(&#34;donated07&#34;,&#34;volume&#34;))) %&gt;% --&gt;
&lt;!--   lof(15) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- Random forest and variable importance measures --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- training$volume &lt;- NULL --&gt;
&lt;!-- rf &lt;- randomForest(factor(donated07) ~ ., data=training, ntree=100, --&gt;
&lt;!--                             keep.forest=TRUE, importance=TRUE) --&gt;
&lt;!-- rf --&gt;
&lt;!-- importance(rf) --&gt;
&lt;!-- varImpPlot(rf) --&gt;
&lt;!-- test &lt;- read.csv(&#34;../../data/test.csv&#34;) %&gt;% --&gt;
&lt;!--   unique() --&gt;
&lt;!-- row.names(test) &lt;- test[,1] --&gt;
&lt;!-- test &lt;- test[-1] --&gt;
&lt;!-- names(test) &lt;- c(&#34;last&#34;, &#34;number&#34;, &#34;volume&#34;, &#34;first&#34;) --&gt;
&lt;!-- rf_pred &lt;- predict(rf, test) --&gt;
&lt;!-- ``` --&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
